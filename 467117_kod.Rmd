---
title: "Projekt Statystyczna Analiza Danych"
author: "Daniel Zalewski"
date: "23 maja 2024 r."
output: html_document
---

```{r}
# Pobranie bibliotek i ustanowienie ziarna
library(ggplot2)
library(reshape2)
library(caret)
library(glmnet)
library(randomForest)
set.seed(123)
```



```{r}
# Wczytanie danych
x_test <- read.csv('X_test.csv')
x_train <- read.csv('X_train.csv')
y_train <- read.csv('y_train.csv')
```

# Zadanie 1a

```{r}
# Podsumowanie liczby obserwacji i zmiennych
cat("Liczba obserwacji i zmiennych w x_train:", dim(x_train), "\n\n")
cat("Liczba obserwacji i zmiennych w x_test:", dim(x_test), "\n\n")
cat("Liczba obserwacji i zmiennych w y_train:", dim(y_train), "\n\n")

# Skrócone sprawdzenie typów danych
cat("Typy danych w x_train:")
print(table(sapply(x_train, class)))
cat("Typy danych w x_test:")
print(table(sapply(x_test, class)))
cat("Typy danych w y_train:")
print(table(sapply(y_train, class)))

# Sprawdzenie brakujących danych
cat("Liczba brakujących wartości w x_train:", sum(is.na(x_train)), "\n\n")
cat("Liczba brakujących wartości w x_test:", sum(is.na(x_test)), "\n\n")
cat("Liczba brakujących wartości w y_train:", sum(is.na(y_train)), "\n\n")
```

# Zadanie 1b

```{r}
# Statystyki podstawowe dla y_train
summary_y_train <- summary(y_train$CD36)
cat("Podstawowe statystyki dla y_train:\n")
print(summary_y_train)
```

```{r}
# Histogram
hist(y_train$CD36, main="Histogram zmiennej objaśnianej", ylab="Częstotliwość", xlab="Ilość białka powierzchniowego CD36", col="blue", breaks=30)
```

```{r}
# Wykres gęstości
ggplot(data = y_train, aes(x = CD36)) + 
  geom_histogram(aes(y = ..density..), binwidth = 0.5, fill="blue", color="black") +
  geom_density(alpha=.2, fill="#FF6666") +
  ggtitle("Histogram z wykresem gęstości zmiennej objaśnianej") +
  xlab("Ilość białka powierzchniowego CD36") +
  ylab("Gęstość")
```

# Zadanie 1c

```{r}
# Obliczenie korelacji między zmiennymi objaśniającymi a objaśnianą
correlations <- sapply(x_train, function(x) cor(x, y_train$CD36, use="complete.obs"))

# Wybór 250 zmiennych o najwyższej korelacji
top_variables <- names(sort(abs(correlations), decreasing=TRUE)[1:250])

# Macierz korelacji dla wybranych zmiennych
selected_correlation_matrix <- cor(x_train[,top_variables], use="pairwise.complete.obs")

# Heatmap korelacji
melted_correlation_matrix <- melt(selected_correlation_matrix)
ggplot(melted_correlation_matrix, aes(Var1, Var2, fill=value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", midpoint = 0, limit = c(-1,1)) +
  ggtitle("Heatmapa Korelacji") +
  theme_minimal() +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        plot.title = element_text(hjust = 0.5, size = 14))
```

# Zadanie 2a

```{r}
# QQ plot porównujący y_train z rozkładem normalnym
qqnorm(y_train$CD36, main="QQ plot zmiennej objaśnianej z rozkładem normalnym", xlab = "Kwantyle teoretyczne", ylab = "Kwantyle próbki")
qqline(y_train$CD36, col = "red", lwd = 2)
```


Nie da się odczytać średniej i wariancji rozkładu doświadczalnego wprost z wykresu, gdyż wykres kwantylowy jest narzędziem do oceny zgodności rozkładu danych z rozkładem normalnym, a nie do określania parametrów tego rozkładu.

# Zadanie 2b

```{r}
# Przeprowadzenie testu Andersona-Darlinga
if (!require(nortest)) install.packages("nortest", dependencies=TRUE)
library(nortest)
test_ad <- ad.test(y_train$CD36)
print(test_ad)
```

# Zadanie 2c

```{r}
# Obliczenie korelacji między wszystkimi zmiennymi objaśniającymi a zmienną objaśnianą
correlations <- sapply(x_train, function(x) cor(x, y_train$CD36, use="complete.obs"))

# Wybór zmiennej najbardziej skorelowanej
most_correlated_var_name <- names(which.max(abs(correlations)))
most_correlated_var <- x_train[[most_correlated_var_name]]
```

# Zadanie 2c (i.)

H0: Zmienna objaśniająca pochodzi z rozkładu wykładniczego z parametrem λ = 10.

H1: Zmienna objaśniająca nie pochodzi z rozkładu wykładniczego z parametrem λ = 10.

α = 0.05

Wybrany test - Test Kołmogorowa-Smirnowa (test KS) jest odpowiedni do porównywania rozkładu próbki z określonym rozkładem teoretycznym, gdyż:
- Jest uniwersalny
- Nie wymaga parametrów (nieparametryczny)
- Jest skuteczny w wykrywaniu różnic między empiryczną funkcją dystrybuanty próby a funkcją dystrybuanty teoretycznego rozkładu.

```{r}
#Test Kołmogorowa-Smirnowa
ks_test_result <- ks.test(most_correlated_var, "pexp", rate = 1/10)
print(ks_test_result)
```
Wynik:
D = 0.70347: Wartość D jest duża, co sugeruje znaczące odchylenie między porównywanymi funkcjami dystrybuanty.

p-value < 2.2e-16: P-wartość jest mała, co oznacza, że odrzucamy hipotezę zerową na korzyść hipotezy alternatywnej. 

Wniosek: Dane nie pochodzą z rozkładu wykładniczego z parametrem λ = 10.



# Zadanie 2c (ii.)

Najpierw przeprowadźmy test, aby określić czy nasze zbiory posiadają rozkład normalny - w celu wybrania odpowiedniego testu porównującego (H0 -> t-Studenta dla niezależnych próbek, H1 -> U Manna-Whitneya).

H0: Zbiór x_train oraz x_test mają rozkład normalny.

H1: Zbiór x_train lub x_test nie ma rozkładu normalnego.

α = 0.05

Wybrany test - test Andersona-Darlinga

```{r}
#Test Andersona-Darlinga
ad_train <- ad.test(x_train[[most_correlated_var_name]])
ad_test <- ad.test(x_test[[most_correlated_var_name]])
print(ad_train)
print(ad_test)
```

Wynik:
A = 1008.4, p-value < 2.2e-16: Odrzucamy H0, przyjmujemy H1.
A = 188.57, p-value < 2.2e-16: Odrzucamy H0, przyjmujemy H1.

Wniosek:
Zbiory znacząco odbiegają od rozkładu normalnego, zatem odpowiednim testem porównującym będzie Test U Manna-Whitneya.


Test sprawdzający wzajemne położenie median:

H0: Mediana zmiennej objaśniającej jest równa w zbiorze testowym i treningowym.

H1: Mediana zmiennej objaśniającej różni się między zbiorem testowym a treningowym.

α = 0.05

Wybrany test - Test U Manna-Whitneya

```{r}
# Test U Manna-Whitneya
comparison_test <- wilcox.test(x_train[[most_correlated_var_name]], x_test[[most_correlated_var_name]], exact = FALSE)
print(comparison_test)
```
Wynik:
W = 4200048, p-value = 0.06969: Brak wystarczających dowodów statystycznych aby odrzucić H0.

Wniosek:
Mediana zmiennej objaśniającej jest równa w zbiorze testowym i treningowym.



# Zadanie 3a

ElasticNet jest popularnym modelem regresji stosowanym w uczeniu maszynowym i statystyce, łączącym cechy dwóch innych metod: regresji Lasso i regresji grzbietowej (Ridge Regression). Model ten jest szczególnie przydatny w przypadkach, gdy mamy do czynienia z problemami wielowymiarowości (high-dimensionality) oraz kiedy cechy (zmienne niezależne) są skorelowane.


Parametry Estymowane w ElasticNet:
W ElasticNet, podobnie jak w regresji Lasso i regresji grzbietowej, estymowane są współczynniki dla poszczególnych zmiennych niezależnych w danych. Parametry te to współczynniki βi w modelu liniowym:

y = β0 + β1x1 + β2x2 + … + βnxn + ϵ

gdzie y to zmienna zależna, xi to zmienne niezależne, natomiast ϵ to błąd modelu.


Optymalizowana Funkcja:
Funkcja, która jest optymalizowana w ElasticNet, to suma błędu kwadratowego z regularyzacją zarówno L1 (jak w Lasso), jak i L2 (jak w regresji grzbietowej). Funkcja kosztu modelu ElasticNet wygląda następująco:

Minimize: ∥y−Xβ∥22 + λ(α∥β∥1 + (1−α)/2* ∥β∥22)

gdzie:
∥y−Xβ∥22 - to suma kwadratów błędów,

∥β∥1 - to suma wartości bezwzględnych współczynników (regularyzacja L1),

∥β∥22 - to suma kwadratów współczynników (regularyzacja L2),

λ to parametr określający siłę regularyzacji,

α to parametr mieszania, który określa relatywny wpływ regularyzacji L1 i L2.


Główne hiperparametry w modelu ElasticNet to:


λ: parametr regularyzacji kontrolujący ogólną siłę regularyzacji obu składników.

α: parametr mieszania, który decyduje o proporcji między regularyzacją L1 a L2.


Specjalne przypadki hiperparametrów:
Regresja Lasso: Otrzymujemy ją, gdy α=1. W tym ustawieniu, model stosuje tylko regularyzację L1, co prowadzi do modelu z cechą selekcji zmiennych (zmienne o mniejszym wpływie mogą otrzymać współczynnik zero).

Regresja grzbietowa (Ridge Regression): Otrzymujemy ją, gdy α=0. W takim przypadku, model stosuje wyłącznie regularyzację L2, co pomaga w obchodzeniu się z problemami wielowymiarowości i korelacji między zmiennymi, ale nie prowadzi do wyzerowania współczynników.


# Zadanie 3b

```{r}
y_train <- as.vector(y_train[,1])
# Ustawienie hiperparametrów
alphas <- c(0, 0.5, 1)
lambdas <- c(0.01, 0.1, 1)

# Funkcja do obliczania MSE
calculate_mse <- function(pred, actual) {
  mean((pred - actual)^2)
}

# Przygotowanie struktury do przechowywania wyników
results_train <- data.frame()
results_test <- data.frame()

# 10-krotna walidacja krzyżowa
folds <- createFolds(y_train, k = 10, list = TRUE)

# Główna pętla przez wszystkie kombinacje hiperparametrów
for (alpha in alphas) {
  for (lambda in lambdas) {
    mse_train <- numeric(10)
    mse_test <- numeric(10)
    
    for (i in 1:10) {
      train_index <- folds[[i]]
      test_index <- setdiff(1:nrow(x_train), train_index)
      
      x_train_fold <- x_train[train_index, ]
      y_train_fold <- y_train[train_index]
      x_test_fold <- x_train[test_index, ]
      y_test_fold <- y_train[test_index]

      model <- cv.glmnet(as.matrix(x_train_fold), y_train_fold, alpha = alpha, lambda = lambdas, nfolds = 10)
      
      # Predykcje dla danych treningowych i testowych
      predictions_train <- predict(model, s = lambda, newx = as.matrix(x_train_fold))
      predictions_test <- predict(model, s = lambda, newx = as.matrix(x_test_fold))
      
      mse_train[i] <- calculate_mse(predictions_train, y_train_fold)
      mse_test[i] <- calculate_mse(predictions_test, y_test_fold)
    }
    
    results_train <- rbind(results_train, data.frame(alpha = alpha, lambda = lambda, fold = 1:10, mse = mse_train))
    results_test <- rbind(results_test, data.frame(alpha = alpha, lambda = lambda, fold = 1:10, mse = mse_test))
  }
}
```

Uzasadnienie wyboru zmiennych:
Alpha: Wartości [0, 0.5, 1] zapewniają możliwość oceny zarówno skrajnych przypadków (czyste Lasso i Ridge) jak i stanu pośredniego.

Lambda: Szeroki zakres wartości [1,0.1,0.01] pozwala na ocenę wpływu różnych poziomów siły regularyzacji na model.

Walidacja krzyżowa: Liczba 10 podzbiorów zapewnia dobre oszacowanie błędu generalizacji modelu, jednocześnie utrzymując rozsądny czas obliczeń.


# Zadanie 3c

```{r}
# Kolumna identyfikująca kombinacje alfa i lambda
results_test$combination <- paste("alpha:", results_test$alpha, "lambda:", results_test$lambda)

# Wwykres skrzypcowy
ggplot(results_test, aes(x = combination, y = mse, fill = combination)) +
  geom_violin(trim = FALSE) +
  geom_jitter(height = 0, width = 0.2, alpha = 0.2, color = "black") +
  labs(title = "Wykres skrzypcowy MSE dla różnych kombinacji hiperparametrów (ElasticNet)",
       x = "Kombinacje hiperparametrów",
       y = "Błąd średniokwadratowy (MSE)") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "right") +
  guides(fill = guide_legend(title = "Kombinacja")) +
  scale_fill_discrete(name = "Kombinacja")
```


* Rozkład MSE:

- Kombinacje z alpha = 1 i lambda = 1 mają najwyższe wartości MSE, co wskazuje na najgorszą wydajność modelu w tych przypadkach.

- Kombinacje z alpha = 0.5 i lambda = 1 również wykazują wyższe wartości MSE, ale są one mniejsze w porównaniu do alpha = 1 i lambda = 1.

* Najniższe MSE:

- Najniższe wartości MSE obserwowane są dla kombinacji z alpha = 0 oraz niskimi wartościami lambda (0.01 i 0.1). To sugeruje, że model osiąga najlepszą wydajność przy tych ustawieniach hiperparametrów.

* Wpływ alpha i lambda:

- Alpha = 0, niezależnie od wartości lambda, generalnie prowadzi do niższych wartości MSE, co wskazuje, że regularyzacja L1 może być bardziej skuteczna w tym przypadku.

- Wyższe wartości lambda generalnie prowadzą do wyższych wartości MSE, co wskazuje, że zbyt duża regularyzacja może pogorszyć wydajność modelu.


# Zadanie 3d


```{r}
test_error_en <- mean(results_test$mse)
train_error_en <- mean(results_train$mse)

cat("Błąd testowy:", test_error_en, "Błąd treningowy:", train_error_en)
```

# Zadanie 4a


```{r}
# Ustawienie hiperparametrów
mtry_values <- c(2, 4)
ntree_values <- c(100, 200)
nodesize_values <- c(1, 5)

# Funkcja do obliczania MSE
calculate_mse <- function(pred, actual) {
  mean((pred - actual)^2)
}

# Przygotowanie struktury do przechowywania wyników
results_train_rf <- data.frame()
results_test_rf <- data.frame()

# 10-krotna walidacja krzyżowa
folds <- createFolds(y_train, k = 10, list = TRUE)

# Główna pętla przez wszystkie kombinacje hiperparametrów
for (mtry in mtry_values) {
  for (ntree in ntree_values) {
    for (nodesize in nodesize_values) {
      mse_train <- numeric(10)
      mse_test <- numeric(10)
      
      for (i in 1:10) {
        train_index <- folds[[i]]
        test_index <- setdiff(1:nrow(x_train), train_index)
        
        x_train_fold <- x_train[train_index, ]
        y_train_fold <- y_train[train_index]
        x_test_fold <- x_train[test_index, ]
        y_test_fold <- y_train[test_index]
        
        model <- randomForest(x = x_train_fold, y = y_train_fold, mtry = mtry, ntree = ntree, nodesize = nodesize)
        
        # Predykcje dla danych treningowych i testowych
        predictions_train <- predict(model, newdata = x_train_fold)
        predictions_test <- predict(model, newdata = x_test_fold)
        
        mse_train[i] <- calculate_mse(predictions_train, y_train_fold)
        mse_test[i] <- calculate_mse(predictions_test, y_test_fold)
      }
      
      results_train_rf <- rbind(results_train_rf, data.frame(mtry = mtry, ntree = ntree, nodesize = nodesize, fold = 1:10, mse = mse_train))
      results_test_rf <- rbind(results_test_rf, data.frame(mtry = mtry, ntree = ntree, nodesize = nodesize, fold = 1:10, mse = mse_test))
    }
  }
}
```


# Zadanie 4b

```{r}
# Boxplot dla danych testowych
results_test_rf$combination <- paste("mtry:", results_test_rf$mtry, "ntree:", results_test_rf$ntree, "nodesize:", results_test_rf$nodesize)

ggplot(results_test_rf, aes(x = combination, y = mse, fill = combination)) +
  geom_boxplot() +
  geom_jitter(height = 0, width = 0.2, alpha = 0.2, color = "black") +
  labs(title = "Wykres pudełkowy MSE dla różnych kombinacji hiperparametrów (Random Forest)",
       x = "Kombinacje hiperparametrów",
       y = "Średni błąd kwadratowy (MSE)") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "right") +
  guides(fill = guide_legend(title = "Kombinacje hiperparametrów"))
```

* Najlepsze wyniki:

- Kombinacje mtry: 4, ntree: 100, nodesize: 5 oraz mtry: 4, ntree: 200, nodesize: 5 mają najniższe wartości MSE, co sugeruje, że te ustawienia hiperparametrów prowadzą do lepszego dopasowania modelu.

* Średnie wyniki:

- Kombinacje mtry: 2, ntree: 100, nodesize: 1, mtry: 2, ntree: 100, nodesize: 5, mtry: 2, ntree: 200, nodesize: 1 i mtry: 2, ntree: 200, nodesize: 5 mają wyższe wartości MSE, ale nadal są stosunkowo niskie.

* Najgorsze wyniki:

- Kombinacje mtry: 2, ntree: 200, nodesize: 1 oraz mtry: 2, ntree: 200, nodesize: 5 mają najwyższe wartości MSE, co sugeruje, że te ustawienia hiperparametrów prowadzą do gorszego dopasowania modelu.


# Zadanie 4c

```{r}
test_error_rf <- mean(results_test_rf$mse)
train_error_rf <- mean(results_train_rf$mse)

cat("Błąd testowy:", test_error_rf, "Błąd treningowy:", train_error_rf)
```

# Zadanie 5

```{r}
# Funkcja do obliczania MSE
calculate_mse <- function(pred, actual) {
  mean((pred - actual)^2)
}

# Przygotowanie struktury do przechowywania wyników
results_train_mm <- data.frame()
results_test_mm <- data.frame()

# 10-krotna walidacja krzyżowa
folds <- createFolds(y_train, k = 10, list = TRUE)

# Pętla przez wszystkie foldy
for (i in 1:10) {
  train_index <- folds[[i]]
  test_index <- setdiff(1:nrow(x_train), train_index)
  
  x_train_fold <- x_train[train_index, ]
  y_train_fold <- y_train[train_index]
  x_test_fold <- x_train[test_index, ]
  y_test_fold <- y_train[test_index]
  
  # Obliczenie średniej arytmetycznej zmiennej objaśnianej w foldzie treningowym
  mean_train <- mean(y_train_fold)
  
  # Przypisanie średniej do predykcji dla danych treningowych i testowych
  predictions_train <- rep(mean_train, length(y_train_fold))
  predictions_test <- rep(mean_train, length(y_test_fold))
  
  # Obliczanie MSE
  mse_train <- calculate_mse(predictions_train, y_train_fold)
  mse_test <- calculate_mse(predictions_test, y_test_fold)
  
  # Dodanie wyników do tabeli
  results_train_mm <- rbind(results_train_mm, data.frame(fold = i, mse = mse_train))
  results_test_mm <- rbind(results_test_mm, data.frame(fold = i, mse = mse_test))
}
```

```{r}
test_error_mm <- mean(results_test_mm$mse)
train_error_mm <- mean(results_train_mm$mse)

cat("Błąd testowy:", test_error_mm, "Błąd treningowy:", train_error_mm)
```

```{r}
# Przygotowanie danych
results_summary <- data.frame(
  Model = c("ElasticNet", "Random Forest", "Mean Model"),
  Mean_Training_Error = c(train_error_en, train_error_rf, train_error_mm),
  Mean_Validation_Error = c(test_error_en, test_error_rf, test_error_mm)
)

# Wyświetlenie tabeli
print(results_summary)

```


Model ElasticNet ma wyższy błąd treningowy, ale niższy błąd walidacyjny w porównaniu do Random Forest. To sugeruje, że ElasticNet lepiej generalizuje na nowe dane i jest mniej podatny na nadmierne dopasowanie. Random Forest ma lepsze dopasowanie do danych treningowych, ale jego zdolność generalizacji jest ograniczona, co wskazuje na możliwy overfitting.
